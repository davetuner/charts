# Default values for spark.
# This is a YAML-formatted file.
# Declare name/value pairs to be passed into your templates.
# name: value

Spark:
  Path: "/spark"
  
Aws:
  AwsAccessKeyId: YOUR_AWS_ACCESS_KEY
  AwsSecretAccessKey: YOUR_AWS_SECRET_KEY

Pyspark:
  PythonVersion: python3

Master:
  Name: master
  Image: davetuner/spark-master:2.3.1-hadoop2.7
  ImagePullPolicy: Always
  Replicas: 1
  Component: "spark-master"
  Cpu: "100m"
  Memory: "512Mi"
  ServicePort: 7077
  ContainerPort: 7077
  # Set Master JVM memory. Default 1g
  # DaemonMemory: 1g
  ServiceType: LoadBalancer

WebUi:
  Name: webui
  ServicePort: 8080
  ContainerPort: 8080

Worker:
  Name: worker
  Image: davetuner/spark-worker:2.3.1-hadoop2.7
  ImagePullPolicy: Always
  Replicas: 3
  Component: "spark-worker"
  Cpu: "100m"
  Memory: "512Mi"
  ServicePort: 8080
  ContainerPort: 8080
  # Set Worker JVM memory. Default 1g
  # DaemonMemory: 1g
  # Set how much total memory workers have to give executors
  # ExecutorMemory: 1g
  ServiceType: LoadBalancer
  Autoscaling:
    Enabled: true
  ReplicasMax: 10
  CpuTargetPercentage: 50

