# Default values for spark.
# This is a YAML-formatted file.
# Declare name/value pairs to be passed into your templates.
# name: value

Pyspark:
  PythonVersion: python3

Hadoop:
  SecretName: dan-hadoop-core-site

Hive:
  SecretName: dan-hive-site

Aws:
  SecretName: aws-keys-berger

Gcs:
  SecretName: dan-gcs-key

Master:
  Name: spark-master
  Image: davetuner/spark-master:2.3.1-hadoop2.7
  ImagePullPolicy: Always
  Component: "spark-master"
  Cpu: "100m"
  Memory: "512Mi"
  ServicePort: 7077
  ContainerPort: 7077
  DaemonMemory: 2g
  ServiceType: ClusterIP

WebUi:
  Name: webui
  ServicePort: 8080
  ContainerPort: 8080

Rest:
  Name: rest
  ServicePort: 6066
  ContainerPort: 6066

Worker:
  Name: spark-worker
  Image: davetuner/spark-worker:2.3.1-hadoop2.7
  ImagePullPolicy: Always
  Replicas: 3
  Component: "spark-worker"
  Cpu: "100m"
  Memory: "512Mi"
  ServicePort: 8080
  ContainerPort: 8080
  DaemonMemory: 2g
  WorkerMemory: 2g
  ServiceType: ClusterIP
  Autoscaling:
    Enabled: true
  ReplicasMax: 10
  CpuTargetPercentage: 50

SparkUiProxy:
  Name: uiproxy
  Image: davetuner/spark-ui-proxy:0.1
  ImagePullPolicy: Always
  Replicas: 1
  Component: "spark-ui-proxy"
  Cpu: "100m"
  Memory: "512Mi"
  ServicePort: 80
  ContainerPort: 80

  